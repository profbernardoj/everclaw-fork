---
tags: [marketing, flavor, kimiclaw-stage1]
status: complete
---
# KimiClaw — Stage 1 Marketing Plan

**Created:** February 19, 2026
**Status:** Draft

---

## Target Persona

**Primary:** Kimi model users, long-context AI users, researchers, analysts, document-heavy professionals

**Demographics:**
- Age: 25-55
- Technical: Moderate to high (comfortable with AI tools, document analysis)
- Values: Deep research, comprehensive analysis, factual accuracy, efficiency
- Pain points: Context window limits, losing thread in long documents, citation tracking, research organization

**Psychographic Profile:**
- "I need AI that can read more than 10 pages" mindset
- Researchers, academics, legal professionals, financial analysts
- Heavy document processing needs
- Values accuracy and depth over speed
- Willing to pay for quality AI that handles long context

---

## Core Messaging Wedge

### The Problem
You chose Kimi for its massive context window. 1 million tokens of understanding. But every session starts fresh. Your research threads are scattered. Citations are manual. And finding that one insight across 50 PDFs you analyzed last month? Good luck.

### The Solution
**KimiClaw: Your long-context research companion.**

- Remember every document, every conversation, every insight
- Build a research memory that spans sessions
- Auto-cite sources from your document corpus
- Search across months of Kimi conversations
- Run locally — your research, your data, your privacy

### The Hook
> *"Kimi never forgets. Now neither will your research."*

---

## Landing Page Outline

### Hero Section
**Headline:** KimiClaw — Your Long-Context Research Companion

**Subhead:** Built for Kimi users. Remember every document, every insight, every conversation.

**CTA:** [Launch Agent] [See How It Works]

### Problem Section
**Headline:** Long Context, Short Memory

**Body:**
- Kimi reads 1M tokens, but every session starts blank
- Research threads scattered across conversations
- Citations require manual tracking
- Can't find insights from documents you analyzed last month
- The power of long context is wasted on amnesia

### Solution Section
**Headline:** KimiClaw: Research Memory for Kimi

**Features:**
- **Document Memory** — Every PDF you've analyzed, indexed forever
- **Conversation Archive** — Search across months of Kimi chats
- **Citation Engine** — Auto-cite sources from your corpus
- **Research Threads** — Connected insights across documents
- **Long-Context Optimization** — Prep prompts that maximize Kimi's window

### How It Works
1. **Connect** — Your Kimi account or API key
2. **Ingest** — Documents, papers, reports, transcripts
3. **Analyze** — Let Kimi and KimiClaw work together
4. **Remember** — Every insight saved, searchable, citable
5. **Own it** — Runs locally, your data, your research

### Social Proof
- "Finally, Kimi remembers what I asked last month" — [Researcher]
- "My literature review just got 10x faster" — [Academic]
- "Citations are no longer a nightmare" — [Legal professional]

### CTA Section
**Headline:** Long context deserves long memory

**CTA:** [Get KimiClaw]

---

## Tagline Options

1. *Kimi never forgets. Now neither will your research.*
2. *Your long-context research companion.*
3. *1M tokens. Infinite memory.*
4. *Research that remembers.*
5. *Built for Kimi. By Kimi users.*

---

## Content Strategy

### Long-form Content (Blog/Mirror)
- "How to Build a Research Memory System for Kimi"
- "Long-Context AI: Beyond the Context Window"
- "The Citation Problem: How KimiClaw Solves It"
- "1 Million Tokens: What Can You Actually Do With It?"

### Short-form (X)
- Threads: "5 ways to maximize Kimi's 1M token window"
- Research tips: "How I organize 100+ PDFs for Kimi analysis"
- Use cases: "Legal document review with KimiClaw"
- Demos: "Watch Kimi analyze a 500-page report"

### Video
- 60-second: "KimiClaw in 60 seconds"
- Tutorial: "Building a research memory with Kimi"
- Demo: "Finding that one insight from 50 documents"

---

## Referral Tracking

### Tracking Mechanism
- UTM parameters: `?ref=kimiclaw&source=twitter&utm_campaign=launch`
- Unique referral codes for academic influencers, research communities
- Cookie duration: 30 days

### Partner Incentives
- 10% of VM setup fee (Stage 2 referral)
- 5% of ClawBox hardware sale (Stage 4 referral)
- Academic discounts for verified researchers

---

## Launch Sequence

### Pre-Launch (Week 1-2)
- [ ] Secure twitter.com/kimiclaw and @kimiclaw handle
- [ ] Create Discord community (research-focused)
- [ ] Reach out to 5-10 academic/research AI influencers
- [ ] Draft blog post: "Long-Context AI: Beyond the Context Window"

### Launch (Week 3)
- [ ] Landing page live at kimiclaw.com
- [ ] X thread announcement
- [ ] Research community shares
- [ ] Academic influencer endorsements

### Post-Launch (Week 4+)
- [ ] Community testimonials
- [ ] Research use case spotlights
- [ ] Academic partnership announcements
- [ ] Iterate based on researcher feedback

---

## Key Metrics

| Metric | Target (Month 1) |
|--------|------------------|
| Landing page visits | 1,200 |
| Signups/installs | 60 |
| X followers | 600 |
| Community members (Discord) | 120 |
| Referral clicks to Stage 2 (VM) | 30 |

---

## Unique Angles for KimiClaw

### Long-Context Specialist
Position as the only agent built specifically for Kimi's massive context window. "We understand what 1M tokens means."

### Research Memory
"Kimi reads. KimiClaw remembers." Every document, every conversation, indexed forever.

### Citation Engine
Auto-cite from your corpus. Huge value prop for academics and legal professionals.

### Academic Angle
Partner with researchers, offer academic discounts. "Built for people who read for a living."

### Privacy-First Research
Your research documents contain sensitive IP. Run locally, keep it private.

---

## Next Steps
- [ ] Review and approve messaging
- [ ] Assign or create Twitter/X account
- [ ] Draft landing page copy
- [ ] Build landing page
- [ ] Recruit 3-5 researchers/academics for testimonials
- [ ] Identify academic influencer partnerships